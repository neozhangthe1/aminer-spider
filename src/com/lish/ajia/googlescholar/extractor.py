# -*- coding: utf-8 -*-
'''
Runner Platform Module: Extractor
KEG â€¢ elivoa[AT]gmail.com
Time-stamp: "root 2010/10/28 13:04:13"
'''
# hack
import sys
sys.path.append("/Users/ChenWei/gbcrawler/src")

#from runner.proxy import proxy
from com.lish.ajia.googlescholar import models
from com.lish.ajia.util.web import HtmlRetriever
from settings import Settings
from Decorator import ClassListUnicode, ClassUnicode
import os
import re
from com.lish.ajia.googlescholar.pdfsaver import PDFLinkSaver
from com.lish.pyutil.DataUtil import GoogleDataCleaner, URLCleaner
from bs4 import BeautifulSoup, UnicodeDammit

class Extractor:
	'''Extract google scholar information (now citation number).
	'''
	__instance = None

	@staticmethod
	def getInstance():
		if Extractor.__instance is None:
			Extractor.__instance = Extractor()
		return Extractor.__instance

	def __init__(self):
#		self.settings = Settings.getInstance()
#		self.debug = self.settings.debug
#		self.htmlRetriever = HtmlRetriever.getInstance(self.settings.use_proxy)
#		if self.settings.save_pdflink:
#			self.pdfcache = PDFLinkSaver.getInstance()
		self.author = re.compile('<div class="?gs_a"?>([^\\x00]+?) - ', re.I)
		self.pdf_block = re.compile('<div class="?gs_ggs gs_fl"?><a href="?([^\s"]+)?"?[^>]+?><span class="?gs_ctg2"?>\[PDF\]</span>', re.I)
		self.citation_block = re.compile('<div class="?gs_fl"?>.*?</div>', re.I)

	def extract_from_source(self, page_html):
		blocks_html = None
		if page_html is not None and len(page_html) > 0:
			soup = BeautifulSoup(page_html)
			blocks_html = soup.find_all('div', 'gs_r')
		
		if(blocks_html is None):
			print ">"*10 + "(block html is none)" + "<"*10

		models = []
		for block in blocks_html:
			model = self.__extract_googlescholar_result(block)
			if model is not None:
				models.append(model)
		return models


	def getNodesByPersonName(self, names):
		'''Get all models by searching use names, multipage
		@return: all_models - {key_title:[ExtractedModel,...]}
		@param: 
			names - person name
		'''
		if names is None or len(names) == 0:
			return None
		
		if self.debug: 
			print 'Extract by person ' , names

		all_models = {}  # {key_title:[ExtractedModel,...]}
		max_pages = 15	#int($total_size * 3 / 100 + 0.5);
		page = 0
		
		for page in range(0, max_pages):
			url, html = self.__getNodesByPersonAndPage(names, page)
#			print page, url, html
			print "Getting page %s, url : [%s]" % (page, url)
			
			if html is None:
				print "Get Page %s failed." % page 
				continue
			
			models = self.extract_from_source(html)
			
			if models is None: continue
			
			self.__merge_into_extractedmap(all_models, models)

			# save source?
			if self.settings.save_source:
				filename = "".join((','.join(names), '_page_', str(page), '.html'))
				f = file(os.path.join(self.settings.source_dir, filename), 'w')
				f.write(url)
				f.write("\n")
				f.write(html)
				f.close()

			itemsPerPage = len(models)
			print "{+A}[Download Page %s, got %s items.] '%s'" % (page, itemsPerPage, names)
			if itemsPerPage < 60:
				break

		if self.debug : 
			print "{+A}[Total found %s items] '%s'" % (len(all_models), names)
		return all_models

	# for test?
	def getNodesByPubs(self, pubs):
		'''Get by pubs.
		Return: 
			all_models, {key_title:[ExtractedModel,...]}, can be None, or []
		Param: 
			pubs, [models.Publication], query generated by this pubs must less than 256.
		'''
		query, used_pubs, nouse_pubs = Extractor.pinMaxQuery(pubs)
		url = self.settings.urltemplate_by_pubs % URLCleaner.encodeUrlForDownload(query)
		message =  "Search Pub[%s] by url[%s]\n" % (used_pubs[0], url)
		# url = URLCleaner.encodeUrlForDownload(url)
		
		html = self.htmlRetriever.getHtmlRetry(url)
		if html is None:
			print "Download Page failed."
			return None
		
		models = self.extract_from_source(html)
		if models is None or len(models) == 0: 
			return None
		# save models
		all_models = self.__merge_into_extractedmap(None, models)  # {key_title:[ExtractedModel,...]}
		for k, v in all_models.items():
			message += "models find: [%s]\n" % v
		print message
		return all_models

	def __extract_googlescholar_result(self, block_html):
		gs_result = models.ExtractedGoogleScholarResult()
		########## title and url
		title_url_block = block_html.find_all('h3', 'gs_rt')
		if title_url_block is None or len(title_url_block) == 0:
			return
		(title, url) = self.get_title_and_url(title_url_block[0])
		if title is None or url is None:
			return
		(readable_title, title_cleaned, has_dot) = self.clean_google_title(title)
		gs_result.title = title
		gs_result.readable_title = readable_title
		gs_result.shunked_title = title_cleaned
		gs_result.title_has_dot = has_dot
		gs_result.web_url = str(url)
		########## citation
		citation_block = re.findall(self.citation_block, str(block_html))
		if citation_block is not None and len(citation_block) > 0:
			gs_result.ncitation = self.get_citation(citation_block[0])
		########## authors
		author_block = block_html.find_all('div', 'gs_a')
		if author_block is not None and len(author_block) > 0:
			authors = re.findall(self.author, str(author_block[0]))
			if authors is not None and len(authors) > 0:
				gs_result.authors = self.clean_authors(authors[0])
	
		########## pdf link
		if True or self.settings.save_pdflink:
			link = re.findall(self.pdf_block, str(block_html))
			if link is not None and len(link) > 0:
				gs_result.pdfLink = link
		return gs_result

	@ClassUnicode
	def clean_authors(self, authors):
		"""
		<a href="as">J Tang</a>, J Li, B Liang, X Huang, Y Li
		"""
		dots = re.compile("(&hellip;)|(\\xe2\\x80\\xa6)")
		pat = re.compile("<.+?>")
		authors = dots.sub("", authors)
		authors = pat.sub(" ", authors)
		return ','.join([re.sub("\\s+", " ", author.strip()) for author in authors.split(',')])
	
	def get_citation(self, citation_block):
		citation = re.compile('<div class="?gs_fl"?>(<a[^>]+?>)?Cited by (\d+)(</a>)?', re.I)
		citation_match = re.findall(citation, str(citation_block))
		if citation_match is None or len(citation_match) == 0:
			return 0
		else:
			return int(citation_match[0][1])
		
	@ClassListUnicode	
	def get_title_and_url(self, head_block):
		"""
		<h3><a></a></h3>
		"""
		try:
			soup = head_block
			if soup.a is not None:
				url = soup.a['href'].strip()
				title = soup.a.get_text().strip()
				return title, url
			elif soup is not None:
				title = re.sub('\[[a-zA-Z]*\]', '', soup.get_text()).strip()
				return title, ''
		except:
			print '[ERROR]Can not parse it using BeautifulSoup'
			print '[SoupLog]', head_block
			return (None, None)
		
	def clean_google_title(self, title):
		has_dot = False
		
		titleCleaned = UnicodeDammit(title).unicode_markup
		# clean step 1
		# BUGFIX: don't remove [xxx]. eg: "OQL[C++]: Ext...'
		titleCleaned = re.sub("(<(.*?)>)", "", titleCleaned)
		re_hasdot = re.compile("(\.\.\.|&hellip;)", re.I)
		match = re_hasdot.search(title)
		if match is not None:
			has_dot = True
			# clean step 2, here title is readable
		titleCleaned = re.sub("(&nbsp;|&#x25ba;|&hellip;)", "", titleCleaned)
		titleCleaned = re.sub("(&#.+?;|&.+?;)", "", titleCleaned)
		titleCleaned = titleCleaned.strip()
		readableTitle = titleCleaned
		# Shrink, only letters left
		titleCleaned = re.sub("\W", "", titleCleaned)
		titleCleaned = titleCleaned.lower()
		return (readableTitle, titleCleaned, has_dot)
				
	def __getNodesByPersonAndPage(self, names, page):
		'''get page# of person, put pubs who found citation into self.found
		Return url, html
		'''
		assert names is not None
		assert page >= 0 and page < 20
		namesInUrl = []
		for name in names:
			namesInUrl.append(("author:%%22%s%%22" % ("+".join(name.strip().split(" ")))));
		
		start = 100 * page
		url = self.settings.urltemplate_by_person_page % (start, '%20OR%20'.join(namesInUrl))
		
		html = self.htmlRetriever.getHtmlRetry(url, with_proxy=True)
		return url, html

	def __merge_into_extractedmap(self, out_all_models, models):
		'''Add all in list models into out_all_models'''
		if out_all_models is None : out_all_models = {}
		for model in models:
			keytitle = model.shunked_title
			if keytitle not in out_all_models:
				out_all_models[keytitle] = [model]
			else:
				models = out_all_models[keytitle]
				models.append(model)
		return out_all_models

	@staticmethod
	def pinMaxQuery(pubs):
		'''Query google scholar use: "xxx a" OR "xxx b" OR ..., max 256 chars. 
		return: query, used_pubs, nouse_pubs(write citation to -10 back to db)
		'''
		printout = False
		maxchars = 256
		query = ""
		total_pubs_used = 0
		used_pubs = []
		nouse_pubs = []
		total_title_length = 0
		for pub in pubs:
			# clean title
			cleaned_titles = GoogleDataCleaner.cleanGoogleTitle(pub.title)
			cleaned_title = cleaned_titles[0]
			
			# Add by gb Nov 05, 2011, filter out nouse titles.
			if cleaned_title is None or len(re.split('[\W+]', cleaned_title)) < 3:
				print "**** no-use-title: ", cleaned_title
				pub.ncitation = -1;
				nouse_pubs.append(pub)
				continue
			
			
			# calc new length
			new_length = Extractor.__calc_control_char_length(total_pubs_used + 1) + total_title_length + len(cleaned_title)

			# 
			splits = cleaned_title.split("\\s+")
			if splits is not None and len(splits) > 1:
				if total_pubs_used == 0:  # if the first one-word paper, only get this.
					new_length += 255
				else:  # skip this one.
					continue

			# first pub must be here, to avoid first pub title length > 255
			if total_pubs_used > 0 and new_length > maxchars:
				break # overflow
			# real pin
			if total_pubs_used > 0:
				query += 'OR'
			query += ''.join(('"', cleaned_title, '"'))
			used_pubs.append(pub)
			total_pubs_used += 1
			total_title_length += len(cleaned_title)
		if printout:# DEBUG PRINT
			blue_temple = "\033[34m%s\033[0m"
			print blue_temple % 'pin query done'
			q = ('query(%s): %s' % (len(query), query))
			print blue_temple % q
			t = 'use %s pubs' % total_pubs_used
			print blue_temple % t
		return query, used_pubs, nouse_pubs


	@staticmethod
	def __calc_control_char_length(numpubs):
		'''Return how many chars used in control character such as '"' 'OR'
		allintitle: 
		'''
		if numpubs <= 0:
			return 0
		return numpubs * 4 - 2

if __name__ == '__main__':
#	url = "http://scholar.google.com/scholar?start=%s&q=%s&hl=en&num=100" % (0, "jiawei han")
#	import urllib
	html = open("/Users/ChenWei/a.html").read()
	e = Extractor()
	print "test"
	models =  e.extract_from_source(html)
	for model in models:
		print model
		print '\n'*2
	

